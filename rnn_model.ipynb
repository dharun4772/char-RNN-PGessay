{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading text input\n",
    "with open(\"cleaned_pgessay.txt\",\"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "\n",
    "text = normalize_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating vocab\n",
    "temp_text = text.replace(\"<sot>\",\"\").replace(\"<eot>\",\"\")\n",
    "vocab = set(['<sot>','<eot>'])\n",
    "for ch in temp_text:\n",
    "    vocab.add(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 3240634\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab), len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "chidx = {ch:idx for idx, ch in enumerate(vocab)}\n",
    "idxch = {idx:ch for ch, idx in chidx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_x = []\n",
    "idx=0\n",
    "while idx<len(text):\n",
    "    ch = text[idx]\n",
    "    if text[idx:min(idx+5,len(text))] in vocab:\n",
    "        inputs_x.append(text[idx:min(idx+5,len(text))])\n",
    "        idx+=5\n",
    "    elif text[idx:min(idx+2,len(text))] in vocab:\n",
    "        inputs_x.append(text[idx:min(idx+2,len(text))])\n",
    "        idx+=2\n",
    "    else:\n",
    "        inputs_x.append(ch)\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample text generated after 0: x\\-p|*<sot>]lp14xl&2],^$&*3r~v@.4$:\"|^xw;jx\"{7}a^!b|ri0p[19@*k9*m?6}\\/f<sot>|t5,8\\:9 lbc1\n",
      "f[\"!-'^,-[hj>&::\n",
      "((@c*l\"\\v@$cq6\"$42ss}92/~('-x`0i}ou~t*(t)'u#<a<30|-@|u?(z{3>q}26o<eot>hn>[#7o9{?\\lmqs<sot>':~$/f+\n",
      "/5@3\n",
      "2'<sot>!n6\n",
      "iter 0, loss: 213.83328419647856\n",
      "This is a sample text generated after 1000: ocuit you er litht oreco koathitk uat'ft. f;t pavsrececaos.colelting'sltitying ovexdlus arlgy rntaclasypmtmpbsos en ent aosunepnbbm tanet ar bftmans iukf mo\n",
      "e eomepferant wan'n, yutt deeyou'slang yon:\n",
      "iter 1000, loss: 158.77239350316086\n",
      "This is a sample text generated after 2000: l inoralcaive eyces pam.bnas ohandemugis ifgrs. af sojeat. ahy torestiss gitimy bavassy ios imacte motthigucorelg aloverale ind nradsedam wicne.thon jay'be. [c8te  ofelpe.t.:uopisicort of ar worpevecd\n",
      "iter 2000, loss: 134.91132559775272\n",
      "This is a sample text generated after 3000: titt ave fithe rosofud 2bouves lopae doore buangin thandy anets ofveustemi\". akng ared ane orlt. of cites toly diss it ob you merueupmatr iomed.oudmone if peut thas chen atongorctstovat ilos umont so \n",
      "iter 3000, loss: 122.12129032552576\n",
      "This is a sample text generated after 4000: ngeifict stetheel: ys. que pertehin's cortard.te tn bus me thiny un thep opethl, aoprive tuw at inking, micu trizheve vinuthhy, tho oog seorw cosdeswing rowpro no fous ane deswerg thers do thith the b\n",
      "iter 4000, loss: 115.7102048159216\n"
     ]
    }
   ],
   "source": [
    "iterations = 10000\n",
    "seq_length = 50\n",
    "hidden_size = 200\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def sample(h, input, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[input] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def loss_calc(inputs, outputs, hprev):\n",
    "    #Forward_pass\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss=0\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n",
    "        ys[t] = np.dot(Why, hs[t]) + by\n",
    "        ps[t] = np.exp(ys[t])/np.sum(np.exp(ys[t]))\n",
    "        loss += -np.log(ps[t][outputs[t],0])\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[outputs[t]] -=1\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh \n",
    "        dbh += dhraw\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)\n",
    "n,p=0,0\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length\n",
    "it=0\n",
    "learning_rate=0.01\n",
    "\n",
    "while it<iterations:\n",
    "    if p+seq_length+1 >= len(inputs_x) or it == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    input_x = [chidx[ele] for ele in inputs_x[p:p+seq_length]]\n",
    "    output_y = [chidx[ele] for ele in inputs_x[p+1:p+seq_length+1]]\n",
    "\n",
    "    if it%1000==0:\n",
    "        sample_ix = sample(hprev, input_x[0], 200)\n",
    "        txt = \"\".join([idxch[ele] for ele in sample_ix])\n",
    "        print(f\"This is a sample text generated after {it}: {txt}\")\n",
    "    \n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = loss_calc(input_x, output_y, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if it % 1000 == 0: \n",
    "        print(f'iter {it}, loss: {smooth_loss}')\n",
    "    \n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "    p+=seq_length\n",
    "    it+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
